"""
üöÄ Comprehensive Multivariate Analysis
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑: PCA + –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ + –ë–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã

–ê–≤—Ç–æ—Ä: Analytics Course
–ì–ª–∞–≤–∞: 24 - –ú—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
plt.style.use('default')
sns.set_palette("husl")

class ComprehensiveAnalyzer:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –º—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    
    def __init__(self):
        self.products_data = None
        self.sales_data = None
        self.merged_data = None
        self.features = None
        self.features_scaled = None
        self.pca_model = None
        self.fa_model = None
        self.pca_components = None
        self.clusters = None
        self.feature_names = None
        
    def load_and_merge_data(self, products_file='products_analysis.csv', 
                           sales_file='sales_data.csv'):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö –∏ –ø—Ä–æ–¥–∞–∂–∞—Ö
        """
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.products_data = pd.read_csv(products_file, sep=';', decimal=',')
        self.sales_data = pd.read_csv(sales_file, sep=';', decimal=',')
        
        print(f"‚úÖ –¢–æ–≤–∞—Ä—ã: {len(self.products_data)} –∑–∞–ø–∏—Å–µ–π")
        print(f"‚úÖ –ü—Ä–æ–¥–∞–∂–∏: {len(self.sales_data)} –∑–∞–ø–∏—Å–µ–π")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        self.merged_data = self.products_data.merge(
            self.sales_data, on='product_id', how='inner'
        )
        
        print(f"üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(self.merged_data)} –∑–∞–ø–∏—Å–µ–π")
        
        return self.merged_data
    
    def feature_engineering(self):
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        print("\nüîß –ò–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.merged_data['profit_margin'] = (
            (self.merged_data['price'] - self.merged_data['cost']) / 
            self.merged_data['price']
        )
        
        self.merged_data['sales_velocity'] = (
            self.merged_data['units_sold'] / self.merged_data['days_in_catalog']
        )
        
        self.merged_data['revenue_per_day'] = (
            self.merged_data['revenue'] / self.merged_data['days_in_catalog']
        )
        
        self.merged_data['marketing_efficiency'] = (
            self.merged_data['revenue'] / self.merged_data['marketing_spend']
        )
        
        self.merged_data['review_density'] = (
            self.merged_data['review_count'] / self.merged_data['days_in_catalog']
        )
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º —Å–∏–ª—å–Ω–æ —Å–∫–æ—à–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        self.merged_data['log_price'] = np.log(self.merged_data['price'])
        self.merged_data['log_units_sold'] = np.log(self.merged_data['units_sold'] + 1)
        
        print("‚úÖ –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        new_features = ['profit_margin', 'sales_velocity', 'revenue_per_day', 
                       'marketing_efficiency', 'review_density', 'log_price', 'log_units_sold']
        for feature in new_features:
            print(f"   ‚Ä¢ {feature}")
        
        return self.merged_data
    
    def prepare_analysis_features(self):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        print("\nüìã –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analysis_features = [
            'log_price', 'profit_margin', 'log_units_sold', 'sales_velocity',
            'return_rate', 'rating', 'review_density', 'category_popularity',
            'seasonal_factor', 'competition_level', 'revenue_per_day',
            'marketing_efficiency', 'conversion_rate'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        available_features = [f for f in analysis_features if f in self.merged_data.columns]
        print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(available_features)}):")
        for feature in available_features:
            print(f"   ‚Ä¢ {feature}")
        
        self.features = self.merged_data[available_features]
        self.feature_names = available_features
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        print(f"\nüîç –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {self.features.isnull().sum().sum()}")
        print(f"   –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {np.isinf(self.features.values).sum()}")
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π
        if self.features.isnull().sum().sum() > 0:
            self.features = self.features.fillna(self.features.median())
            print("   ‚úÖ –ü—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
        
        return self.features
    
    def standardize_data(self):
        """
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ
        """
        print("\n‚öñÔ∏è –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        
        scaler = StandardScaler()
        self.features_scaled = scaler.fit_transform(self.features)
        
        print("‚úÖ –î–∞–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        return self.features_scaled
    
    def perform_comprehensive_pca(self, variance_threshold=0.85):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ PCA
        """
        print("\nüîç –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        pca_full = PCA()
        pca_full.fit(self.features_scaled)
        
        cumsum = np.cumsum(pca_full.explained_variance_ratio_)
        n_components = np.argmax(cumsum >= variance_threshold) + 1
        
        print(f"üéØ –î–ª—è {variance_threshold:.0%} –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ {n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º PCA
        self.pca_model = PCA(n_components=n_components)
        self.pca_components = self.pca_model.fit_transform(self.features_scaled)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã PCA:")
        print(f"   –ö–æ–º–ø–æ–Ω–µ–Ω—Ç: {self.pca_model.n_components_}")
        print(f"   –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å: {self.pca_model.explained_variance_ratio_.sum():.1%}")
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._interpret_pca_components()
        
        return self.pca_components
    
    def _interpret_pca_components(self, top_n=3):
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç –≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        """
        print("\nüéØ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç:")
        
        loadings = self.pca_model.components_.T
        
        for i in range(self.pca_model.n_components_):
            variance = self.pca_model.explained_variance_ratio_[i]
            print(f"\nüìä PC{i+1} (–æ–±—ä—è—Å–Ω—è–µ—Ç {variance:.1%} –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏):")
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –Ω–∞–≥—Ä—É–∑–∫–∞–º–∏
            component_loadings = loadings[:, i]
            abs_loadings = np.abs(component_loadings)
            top_indices = np.argsort(abs_loadings)[-top_n:][::-1]
            
            print("   –ö–ª—é—á–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:")
            for idx in top_indices:
                feature = self.feature_names[idx]
                loading = component_loadings[idx]
                direction = "‚¨ÜÔ∏è" if loading > 0 else "‚¨áÔ∏è"
                print(f"     {direction} {feature}: {loading:.3f}")
            
            # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é
            interpretation = self._suggest_component_interpretation(
                [self.feature_names[idx] for idx in top_indices],
                [component_loadings[idx] for idx in top_indices]
            )
            print(f"   üí° –í–æ–∑–º–æ–∂–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {interpretation}")
    
    def _suggest_component_interpretation(self, top_features, top_loadings):
        """
        –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ø-–ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        """
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —Ç–µ–º—ã
        themes = {
            'price': ['log_price', 'profit_margin'],
            'performance': ['log_units_sold', 'sales_velocity', 'revenue_per_day'],
            'quality': ['rating', 'return_rate', 'review_density'],
            'market': ['competition_level', 'category_popularity', 'seasonal_factor'],
            'efficiency': ['marketing_efficiency', 'conversion_rate']
        }
        
        theme_scores = {}
        for theme, keywords in themes.items():
            score = 0
            for feature, loading in zip(top_features, top_loadings):
                for keyword in keywords:
                    if keyword in feature:
                        score += abs(loading)
            theme_scores[theme] = score
        
        # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —Ç–µ–º—É
        if theme_scores:
            dominant_theme = max(theme_scores, key=theme_scores.get)
            interpretations = {
                'price': '"–¶–µ–Ω–æ–≤–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"',
                'performance': '"–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"',
                'quality': '"–ö–∞—á–µ—Å—Ç–≤–æ –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ"',
                'market': '"–†—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è"',
                'efficiency': '"–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"'
            }
            return interpretations.get(dominant_theme, '"–°–º–µ—à–∞–Ω–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä"')
        else:
            return '"–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"'
    
    def perform_clustering_analysis(self, max_clusters=8):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        """
        print("\nüë• –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        silhouette_scores = []
        K_range = range(2, max_clusters + 1)
        
        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42)
            cluster_labels = kmeans.fit_predict(self.pca_components)
            silhouette_avg = silhouette_score(self.pca_components, cluster_labels)
            silhouette_scores.append(silhouette_avg)
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        optimal_k = K_range[np.argmax(silhouette_scores)]
        best_score = max(silhouette_scores)
        
        print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_k}")
        print(f"üìä Silhouette score: {best_score:.3f}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
        kmeans = KMeans(n_clusters=optimal_k, random_state=42)
        self.clusters = kmeans.fit_predict(self.pca_components)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
        self._analyze_clusters()
        
        return self.clusters
    
    def _analyze_clusters(self):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        print("\nüìä –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        analysis_data = self.merged_data.copy()
        analysis_data['cluster'] = self.clusters
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        key_metrics = ['price', 'units_sold', 'revenue', 'profit_margin', 
                      'rating', 'return_rate', 'category_popularity']
        
        cluster_profiles = analysis_data.groupby('cluster')[key_metrics].agg({
            'price': ['mean', 'median'],
            'units_sold': ['mean', 'median'],
            'revenue': ['mean', 'median'],
            'profit_margin': ['mean'],
            'rating': ['mean'],
            'return_rate': ['mean'],
            'category_popularity': ['mean']
        }).round(2)
        
        print(cluster_profiles)
        
        # –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_sizes = analysis_data['cluster'].value_counts().sort_index()
        print(f"\nüìä –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
        for cluster, size in cluster_sizes.items():
            percentage = size / len(analysis_data) * 100
            print(f"   –ö–ª–∞—Å—Ç–µ—Ä {cluster}: {size} —Ç–æ–≤–∞—Ä–æ–≤ ({percentage:.1f}%)")
        
        return cluster_profiles
    
    def generate_business_insights(self):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        """
        print("\n" + "="*60)
        print("üí° –ë–ò–ó–ù–ï–°-–ò–ù–°–ê–ô–¢–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        print("="*60)
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        analysis_data = self.merged_data.copy()
        analysis_data['cluster'] = self.clusters
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ –ø—Ä–∏–±—ã–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
        cluster_revenue = analysis_data.groupby('cluster')['revenue'].sum()
        cluster_profit = analysis_data.groupby('cluster').apply(
            lambda x: ((x['price'] - x['cost']) * x['units_sold']).sum()
        )
        
        top_revenue_cluster = cluster_revenue.idxmax()
        top_profit_cluster = cluster_profit.idxmax()
        
        print(f"üèÜ –¢–æ–ø-–∫–ª–∞—Å—Ç–µ—Ä –ø–æ –≤—ã—Ä—É—á–∫–µ: –ö–ª–∞—Å—Ç–µ—Ä {top_revenue_cluster}")
        print(f"üí∞ –¢–æ–ø-–∫–ª–∞—Å—Ç–µ—Ä –ø–æ –ø—Ä–∏–±—ã–ª–∏: –ö–ª–∞—Å—Ç–µ—Ä {top_profit_cluster}")
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
        low_performers = analysis_data[
            (analysis_data['sales_velocity'] < analysis_data['sales_velocity'].quantile(0.25)) |
            (analysis_data['profit_margin'] < analysis_data['profit_margin'].quantile(0.25))
        ]
        
        print(f"\n‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã: {len(low_performers)} ({len(low_performers)/len(analysis_data)*100:.1f}%)")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        print("\nüéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
        
        for cluster in sorted(analysis_data['cluster'].unique()):
            cluster_data = analysis_data[analysis_data['cluster'] == cluster]
            avg_profit = cluster_data['profit_margin'].mean()
            avg_sales = cluster_data['sales_velocity'].mean()
            avg_rating = cluster_data['rating'].mean()
            
            print(f"\nüìä –ö–ª–∞—Å—Ç–µ—Ä {cluster}:")
            print(f"   –¢–æ–≤–∞—Ä–æ–≤: {len(cluster_data)}")
            print(f"   –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {avg_profit:.1%}")
            print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂: {avg_sales:.2f}")
            print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {avg_rating:.1f}")
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if avg_profit > 0.3 and avg_sales > analysis_data['sales_velocity'].median():
                print("   üöÄ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–∑–≤—ë–∑–¥—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)")
            elif avg_profit > 0.3 and avg_sales <= analysis_data['sales_velocity'].median():
                print("   üìà –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ (—Å–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã)")
            elif avg_profit <= 0.3 and avg_sales > analysis_data['sales_velocity'].median():
                print("   üí° –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞—Ç—Ä–∞—Ç (–ø–æ–ø—É–ª—è—Ä–Ω—ã–µ, –Ω–æ –º–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ)")
            else:
                print("   ‚ö†Ô∏è –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–µ—Ä–µ—Å–º–æ—Ç—Ä –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ (–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ)")
    
    def create_executive_summary(self):
        """
        –°–æ–∑–¥–∞–µ—Ç executive summary –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞
        """
        print("\n" + "="*60)
        print("üìä EXECUTIVE SUMMARY")
        print("="*60)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ü–∏—Ñ—Ä—ã
        total_products = len(self.merged_data)
        total_revenue = self.merged_data['revenue'].sum()
        avg_profit_margin = self.merged_data['profit_margin'].mean()
        
        print(f"üìã –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:")
        print(f"   ‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {total_products:,}")
        print(f"   ‚Ä¢ –û–±—â–∞—è –≤—ã—Ä—É—á–∫–∞: {total_revenue:,.0f} —Ä—É–±.")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {avg_profit_margin:.1%}")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        print(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"   ‚Ä¢ –í—ã–¥–µ–ª–µ–Ω–æ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {self.pca_model.n_components_}")
        print(f"   ‚Ä¢ –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å: {self.pca_model.explained_variance_ratio_.sum():.1%}")
        print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(np.unique(self.clusters))}")
        
        # –¢–æ–ø-–∏–Ω—Å–∞–π—Ç—ã
        print(f"\nüí° –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã:")
        print(f"   1. –ê—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç –º–æ–∂–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–ø–∏—Å–∞—Ç—å {self.pca_model.n_components_} –∫–ª—é—á–µ–≤—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏")
        print(f"   2. –¢–æ–≤–∞—Ä—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è –≤ {len(np.unique(self.clusters))} —Å–µ–≥–º–µ–Ω—Ç–∞")
        print(f"   3. –ö–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π
        print(f"\nüéØ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:")
        print(f"   1. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã")
        print(f"   2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –Ω–∏–∑–∫–æ—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"   3. –ü–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        
        return {
            'total_products': total_products,
            'total_revenue': total_revenue,
            'avg_profit_margin': avg_profit_margin,
            'n_components': self.pca_model.n_components_,
            'explained_variance': self.pca_model.explained_variance_ratio_.sum(),
            'n_clusters': len(np.unique(self.clusters))
        }
    
    def create_comprehensive_visualization(self):
        """
        –°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Scree plot
        ax1 = axes[0, 0]
        pca_full = PCA()
        pca_full.fit(self.features_scaled)
        explained_var = pca_full.explained_variance_ratio_
        
        ax1.plot(range(1, len(explained_var) + 1), explained_var, 'bo-')
        ax1.set_title('üîç Scree Plot')
        ax1.set_xlabel('–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞')
        ax1.set_ylabel('–ò–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å')
        ax1.grid(True, alpha=0.3)
        
        # 2. –ö–ª–∞—Å—Ç–µ—Ä—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ PC1-PC2
        ax2 = axes[0, 1]
        scatter = ax2.scatter(self.pca_components[:, 0], self.pca_components[:, 1], 
                            c=self.clusters, cmap='viridis', alpha=0.7)
        ax2.set_title('üë• –ö–ª–∞—Å—Ç–µ—Ä—ã (PC1 vs PC2)')
        ax2.set_xlabel(f'PC1 ({self.pca_model.explained_variance_ratio_[0]:.1%})')
        ax2.set_ylabel(f'PC2 ({self.pca_model.explained_variance_ratio_[1]:.1%})')
        plt.colorbar(scatter, ax=ax2)
        
        # 3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        ax3 = axes[0, 2]
        cluster_counts = pd.Series(self.clusters).value_counts().sort_index()
        ax3.bar(cluster_counts.index, cluster_counts.values, color='skyblue', alpha=0.7)
        ax3.set_title('üìä –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
        ax3.set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä')
        ax3.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤')
        
        # 4. Heatmap –Ω–∞–≥—Ä—É–∑–æ–∫
        ax4 = axes[1, 0]
        loadings = self.pca_model.components_.T
        n_comp_show = min(4, self.pca_model.n_components_)
        
        im = ax4.imshow(loadings[:, :n_comp_show], cmap='RdBu_r', aspect='auto')
        ax4.set_title('üî• –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–∞–≥—Ä—É–∑–æ–∫')
        ax4.set_xlabel('–ì–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã')
        ax4.set_ylabel('–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ')
        ax4.set_xticks(range(n_comp_show))
        ax4.set_xticklabels([f'PC{i+1}' for i in range(n_comp_show)])
        ax4.set_yticks(range(len(self.feature_names)))
        ax4.set_yticklabels(self.feature_names, fontsize=8)
        plt.colorbar(im, ax=ax4)
        
        # 5. –í—ã—Ä—É—á–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        ax5 = axes[1, 1]
        analysis_data = self.merged_data.copy()
        analysis_data['cluster'] = self.clusters
        revenue_by_cluster = analysis_data.groupby('cluster')['revenue'].sum()
        
        ax5.bar(revenue_by_cluster.index, revenue_by_cluster.values, 
               color='lightcoral', alpha=0.7)
        ax5.set_title('üí∞ –í—ã—Ä—É—á–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º')
        ax5.set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä')
        ax5.set_ylabel('–í—ã—Ä—É—á–∫–∞ (—Ä—É–±.)')
        ax5.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        # 6. –ú–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        ax6 = axes[1, 2]
        margin_by_cluster = analysis_data.groupby('cluster')['profit_margin'].mean()
        
        colors = ['red' if x < 0.2 else 'orange' if x < 0.3 else 'green' 
                 for x in margin_by_cluster.values]
        ax6.bar(margin_by_cluster.index, margin_by_cluster.values, 
               color=colors, alpha=0.7)
        ax6.set_title('üìà –ú–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º')
        ax6.set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä')
        ax6.set_ylabel('–°—Ä–µ–¥–Ω—è—è –º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å')
        ax6.axhline(y=0.2, color='red', linestyle='--', alpha=0.5, label='20%')
        ax6.axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='30%')
        
        plt.tight_layout()
        plt.show()


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    print("="*60)
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = ComprehensiveAnalyzer()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    analyzer.load_and_merge_data()
    
    # –ò–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    analyzer.feature_engineering()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    analyzer.prepare_analysis_features()
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    analyzer.standardize_data()
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º PCA
    analyzer.perform_comprehensive_pca()
    
    # –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    analyzer.perform_clustering_analysis()
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    analyzer.create_comprehensive_visualization()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã
    analyzer.generate_business_insights()
    
    # Executive summary
    summary = analyzer.create_executive_summary()
    
    print("\nüéâ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    return analyzer, summary


if __name__ == "__main__":
    analyzer, summary = main()