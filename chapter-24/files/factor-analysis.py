"""
üß© Factor Analysis Template
–®–∞–±–ª–æ–Ω –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (Factor Analysis)

–ê–≤—Ç–æ—Ä: Analytics Course
–ì–ª–∞–≤–∞: 24 - –ú—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import FactorAnalysis
from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use('default')
sns.set_palette("viridis")

class FactorAnalysisToolkit:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    
    def __init__(self):
        self.data = None
        self.features = None
        self.fa_model = None
        self.loadings = None
        self.feature_names = None
        self.n_factors = None
        
    def load_data(self, file_path, id_column='employee_id'):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        Parameters:
        -----------
        file_path : str
            –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        id_column : str
            –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        """
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.data = pd.read_csv(file_path, sep=';', decimal=',')
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –∑–∞–ø–∏—Å–µ–π —Å {len(self.data.columns)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏")
        
        # –í—ã–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        self.features = self.data.drop(columns=[id_column])
        self.feature_names = list(self.features.columns)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"üìã –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(self.feature_names)}")
        print(f"üîç –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {self.features.isnull().sum().sum()}")
        
        return self.features
    
    def check_data_adequacy(self):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –¢–µ—Å—Ç –ë–∞—Ä—Ç–ª–µ—Ç—Ç–∞ –Ω–∞ —Å—Ñ–µ—Ä–∏—á–Ω–æ—Å—Ç—å
        chi_square_value, p_value = calculate_bartlett_sphericity(self.features)
        print(f"\nüìä –¢–µ—Å—Ç –ë–∞—Ä—Ç–ª–µ—Ç—Ç–∞ –Ω–∞ —Å—Ñ–µ—Ä–∏—á–Ω–æ—Å—Ç—å:")
        print(f"   œá¬≤ = {chi_square_value:.2f}")
        print(f"   p-value = {p_value:.6f}")
        
        if p_value < 0.05:
            print("   ‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (p < 0.05)")
        else:
            print("   ‚ùå –î–∞–Ω–Ω—ã–µ –ù–ï –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (p >= 0.05)")
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π –ö–ú–û (Kaiser-Meyer-Olkin)
        kmo_all, kmo_model = calculate_kmo(self.features)
        print(f"\nüéØ –ö—Ä–∏—Ç–µ—Ä–∏–π –ö–ú–û:")
        print(f"   –û–±—â–∏–π –ö–ú–û = {kmo_model:.3f}")
        
        if kmo_model >= 0.8:
            print("   üèÜ –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å (–ö–ú–û >= 0.8)")
        elif kmo_model >= 0.7:
            print("   ‚úÖ –•–æ—Ä–æ—à–∞—è –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å (–ö–ú–û >= 0.7)")
        elif kmo_model >= 0.6:
            print("   ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å (–ö–ú–û >= 0.6)")
        elif kmo_model >= 0.5:
            print("   ‚ùå –ü–ª–æ—Ö–∞—è –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å (–ö–ú–û >= 0.5)")
        else:
            print("   üíÄ –ù–µ–ø—Ä–∏–µ–º–ª–µ–º–∞—è –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å (–ö–ú–û < 0.5)")
        
        # –ö–ú–û –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        print("\nüìã –ö–ú–û –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
        for i, var in enumerate(self.feature_names):
            kmo_var = kmo_all[i]
            status = "‚úÖ" if kmo_var >= 0.5 else "‚ùå"
            print(f"   {status} {var}: {kmo_var:.3f}")
        
        return chi_square_value, p_value, kmo_model
    
    def determine_n_factors(self, max_factors=None):
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        """
        print("\nüéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤...")
        
        if max_factors is None:
            max_factors = min(len(self.feature_names), len(self.features) // 5)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–µ–∑ –≤—Ä–∞—â–µ–Ω–∏—è
        fa = FactorAnalyzer(rotation=None)
        fa.fit(self.features)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        eigenvalues, v = fa.get_eigenvalues()
        
        # –ü—Ä–∞–≤–∏–ª–æ –ö–∞–π–∑–µ—Ä–∞ (eigenvalue > 1)
        kaiser_n = np.sum(eigenvalues > 1)
        print(f"üìä –ü—Ä–∞–≤–∏–ª–æ –ö–∞–π–∑–µ—Ä–∞ (eigenvalue > 1): {kaiser_n} —Ñ–∞–∫—Ç–æ—Ä–æ–≤")
        
        # Scree plot
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'bo-', linewidth=2, markersize=8)
        plt.axhline(y=1, color='red', linestyle='--', linewidth=2, label='Eigenvalue = 1')
        plt.title('üß© Scree Plot –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞', fontsize=14, fontweight='bold')
        plt.xlabel('–ù–æ–º–µ—Ä —Ñ–∞–∫—Ç–æ—Ä–∞')
        plt.ylabel('–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏
        print("\nüìà –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –ø–æ —Ñ–∞–∫—Ç–æ—Ä–∞–º:")
        cumulative = 0
        for i, ev in enumerate(eigenvalues[:min(10, len(eigenvalues))]):
            variance_explained = ev / len(self.feature_names) * 100
            cumulative += variance_explained
            print(f"   –§–∞–∫—Ç–æ—Ä {i+1}: {variance_explained:.1f}% (–Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è: {cumulative:.1f}%)")
        
        self.n_factors = kaiser_n
        return kaiser_n, eigenvalues
    
    def perform_factor_analysis(self, n_factors=None, rotation='varimax'):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –≤—Ä–∞—â–µ–Ω–∏–µ–º
        
        Parameters:
        -----------
        n_factors : int
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        rotation : str
            –¢–∏–ø –≤—Ä–∞—â–µ–Ω–∏—è ('varimax', 'promax', 'oblimin', None)
        """
        if n_factors is None:
            n_factors = self.n_factors
        
        print(f"\nüß© –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å {n_factors} —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏ (–≤—Ä–∞—â–µ–Ω–∏–µ: {rotation})...")
        
        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        self.fa_model = FactorAnalyzer(n_factors=n_factors, rotation=rotation)
        self.fa_model.fit(self.features)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –Ω–∞–≥—Ä—É–∑–æ–∫
        self.loadings = self.fa_model.loadings_
        
        print(f"‚úÖ –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        communalities = self.fa_model.get_communalities()
        uniquenesses = 1 - communalities
        
        print(f"\nüìä –ö–∞—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è:")
        print(f"   –°—Ä–µ–¥–Ω—è—è –æ–±—â–Ω–æ—Å—Ç—å: {np.mean(communalities):.3f}")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {np.mean(uniquenesses):.3f}")
        
        return self.loadings
    
    def plot_factor_loadings(self, save_path=None):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ñ–∞–∫—Ç–æ—Ä–Ω—ã—Ö –Ω–∞–≥—Ä—É–∑–æ–∫
        """
        loadings_df = pd.DataFrame(
            self.loadings,
            index=self.feature_names,
            columns=[f'–§–∞–∫—Ç–æ—Ä {i+1}' for i in range(self.loadings.shape[1])]
        )
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(loadings_df, annot=True, cmap='RdBu_r', center=0,
                   fmt='.2f', square=True, linewidths=0.5,
                   cbar_kws={'label': '–§–∞–∫—Ç–æ—Ä–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞'})
        plt.title('üî• –ú–∞—Ç—Ä–∏—Ü–∞ —Ñ–∞–∫—Ç–æ—Ä–Ω—ã—Ö –Ω–∞–≥—Ä—É–∑–æ–∫', fontsize=14, fontweight='bold')
        plt.xlabel('–§–∞–∫—Ç–æ—Ä—ã')
        plt.ylabel('–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
        
        return loadings_df
    
    def interpret_factors(self, threshold=0.3):
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç —Ñ–∞–∫—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–≥—Ä—É–∑–æ–∫
        
        Parameters:
        -----------
        threshold : float
            –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        """
        print(f"\nüéØ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤ (–ø–æ—Ä–æ–≥ –Ω–∞–≥—Ä—É–∑–∫–∏: {threshold}):")
        
        loadings_df = pd.DataFrame(
            self.loadings,
            index=self.feature_names,
            columns=[f'–§–∞–∫—Ç–æ—Ä {i+1}' for i in range(self.loadings.shape[1])]
        )
        
        for i in range(self.loadings.shape[1]):
            factor_name = f'–§–∞–∫—Ç–æ—Ä {i+1}'
            print(f"\nüß© {factor_name}:")
            
            # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å –≤—ã—Å–æ–∫–∏–º–∏ –Ω–∞–≥—Ä—É–∑–∫–∞–º–∏
            high_loadings = loadings_df[factor_name][abs(loadings_df[factor_name]) >= threshold]
            high_loadings = high_loadings.sort_values(key=abs, ascending=False)
            
            if len(high_loadings) > 0:
                print(f"   üìä –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å –≤—ã—Å–æ–∫–∏–º–∏ –Ω–∞–≥—Ä—É–∑–∫–∞–º–∏:")
                for var, loading in high_loadings.items():
                    direction = "‚¨ÜÔ∏è" if loading > 0 else "‚¨áÔ∏è"
                    print(f"     {direction} {var}: {loading:.3f}")
                
                # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
                print(f"   üí° –í–æ–∑–º–æ–∂–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:")
                self._suggest_interpretation(high_loadings, factor_name)
            else:
                print(f"   ‚ö†Ô∏è –ù–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –Ω–∞–≥—Ä—É–∑–∫–æ–π –≤—ã—à–µ {threshold}")
        
        return loadings_df
    
    def _suggest_interpretation(self, high_loadings, factor_name):
        """
        –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Ñ–∞–∫—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        """
        variables = high_loadings.index.tolist()
        
        # –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        interpretations = {
            'salary': '–ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –º–æ—Ç–∏–≤–∞—Ü–∏—è',
            'bonus': '–ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –º–æ—Ç–∏–≤–∞—Ü–∏—è', 
            'pay': '–ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –º–æ—Ç–∏–≤–∞—Ü–∏—è',
            'team': '–°–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–º–∞—Ç',
            'manager': '–°–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–º–∞—Ç',
            'culture': '–°–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–º–∞—Ç',
            'relation': '–°–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–º–∞—Ç',
            'career': '–†–∞–∑–≤–∏—Ç–∏–µ –∏ —Ä–æ—Å—Ç',
            'growth': '–†–∞–∑–≤–∏—Ç–∏–µ –∏ —Ä–æ—Å—Ç',
            'training': '–†–∞–∑–≤–∏—Ç–∏–µ –∏ —Ä–æ—Å—Ç',
            'innovation': '–†–∞–∑–≤–∏—Ç–∏–µ –∏ —Ä–æ—Å—Ç',
            'work': '–£—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã',
            'environment': '–£—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã',
            'balance': '–£—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã',
            'stress': '–£—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã'
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        counts = {}
        for var in variables:
            var_lower = var.lower()
            for keyword, interpretation in interpretations.items():
                if keyword in var_lower:
                    counts[interpretation] = counts.get(interpretation, 0) + 1
        
        if counts:
            most_likely = max(counts, key=counts.get)
            print(f"     üéØ –í–µ—Ä–æ—è—Ç–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: '{most_likely}'")
        else:
            print(f"     ü§î –¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏")
    
    def analyze_communalities(self):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        """
        communalities = self.fa_model.get_communalities()
        
        print("\nüìä –ê–Ω–∞–ª–∏–∑ –æ–±—â–Ω–æ—Å—Ç–µ–π (communalities):")
        comm_df = pd.DataFrame({
            '–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è': self.feature_names,
            '–û–±—â–Ω–æ—Å—Ç—å': communalities,
            '–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å': 1 - communalities
        }).sort_values('–û–±—â–Ω–æ—Å—Ç—å', ascending=False)
        
        print(comm_df.round(3).to_string(index=False))
        
        # –í—ã–¥–µ–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        low_comm = comm_df[comm_df['–û–±—â–Ω–æ—Å—Ç—å'] < 0.4]
        if len(low_comm) > 0:
            print("\n‚ö†Ô∏è –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å –Ω–∏–∑–∫–æ–π –æ–±—â–Ω–æ—Å—Ç—å—é (< 0.4):")
            for _, row in low_comm.iterrows():
                print(f"   {row['–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è']}: {row['–û–±—â–Ω–æ—Å—Ç—å']:.3f}")
            print("   üí° –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ —ç—Ç–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–ª–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤")
        
        return comm_df
    
    def create_factor_scores(self):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
        """
        factor_scores = self.fa_model.transform(self.features)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
        scores_df = pd.DataFrame(
            factor_scores,
            columns=[f'–§–∞–∫—Ç–æ—Ä_{i+1}' for i in range(factor_scores.shape[1])]
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π ID
        if hasattr(self, 'data'):
            id_column = self.data.columns[0]  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ - ID
            scores_df[id_column] = self.data[id_column].values
        
        print(f"\nüìä –§–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã –¥–ª—è {len(scores_df)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π")
        print("üìã –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–∫—Ç–æ—Ä–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫:")
        print(scores_df.describe().round(3))
        
        return scores_df
    
    def generate_report(self):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É
        """
        print("\n" + "="*60)
        print("üß© –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–û –§–ê–ö–¢–û–†–ù–û–ú–£ –ê–ù–ê–õ–ò–ó–£")
        print("="*60)
        
        print(f"üìã –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(self.data)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, {len(self.feature_names)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
        print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤: {self.fa_model.n_factors}")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        communalities = self.fa_model.get_communalities()
        mean_communality = np.mean(communalities)
        
        print(f"\nüìä –ö–∞—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è:")
        print(f"   –°—Ä–µ–¥–Ω—è—è –æ–±—â–Ω–æ—Å—Ç—å: {mean_communality:.3f}")
        
        if mean_communality >= 0.6:
            print("   ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è")
        elif mean_communality >= 0.4:
            print("   ‚ö†Ô∏è –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è")
        else:
            print("   ‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è")
        
        # –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        eigenvalues, _ = self.fa_model.get_eigenvalues()
        total_variance = np.sum(eigenvalues[:self.fa_model.n_factors])
        variance_explained = total_variance / len(self.feature_names) * 100
        
        print(f"\nüìà –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è: {variance_explained:.1f}%")
        
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        if mean_communality >= 0.6 and variance_explained >= 60:
            print("   üèÜ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –§–∞–∫—Ç–æ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å")
        elif mean_communality >= 0.4 and variance_explained >= 40:
            print("   ‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —É–ª—É—á—à–µ–Ω–∏—è")
        else:
            print("   üîÑ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")


def main():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    print("üß© –ó–∞–ø—É—Å–∫ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    print("="*50)
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = FactorAnalysisToolkit()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    analyzer.load_data('employee_satisfaction.csv')
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
    analyzer.check_data_adequacy()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
    analyzer.determine_n_factors()
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    analyzer.perform_factor_analysis(rotation='varimax')
    
    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–≥—Ä—É–∑–∫–∏
    loadings_df = analyzer.plot_factor_loadings()
    
    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º —Ñ–∞–∫—Ç–æ—Ä—ã
    analyzer.interpret_factors(threshold=0.3)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â–Ω–æ—Å—Ç–∏
    analyzer.analyze_communalities()
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
    factor_scores = analyzer.create_factor_scores()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    analyzer.generate_report()
    
    print("\nüéâ –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return analyzer, factor_scores


if __name__ == "__main__":
    analyzer, scores = main()