"""
üöÄ Production-ready —Å–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:
- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å MLPipeline –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π feature engineering –∏ data quality checks
- –°–∏—Å—Ç–µ–º—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ model versioning
- Batch prediction –∏ monitoring capabilities
- A/B testing framework –∏ business metrics
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                           roc_auc_score, mean_squared_error, mean_absolute_error, r2_score)
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from datetime import datetime, timedelta
import joblib
import warnings
import json
import os

warnings.filterwarnings('ignore')

print("üöÄ Production-ready —Å–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è!")
print("=" * 50)

class MLPipeline:
    """
    Production-ready –ø–∞–π–ø–ª–∞–π–Ω –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—é
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π feature engineering
    - Model selection –∏ hyperparameter tuning
    - –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    - Batch prediction
    - Monitoring –∏ drift detection
    """
    
    def __init__(self, task_type='classification', random_state=42):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –ø–∞–π–ø–ª–∞–π–Ω–∞
        
        Parameters:
        -----------
        task_type : str
            'classification' –∏–ª–∏ 'regression'
        random_state : int
            –§–∏–∫—Å–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
        """
        self.task_type = task_type
        self.random_state = random_state
        
        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –æ–±—ä–µ–∫—Ç—ã
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        self.preprocessor = None
        self.feature_names = None
        self.target_column = None
        
        # –°–∏—Å—Ç–µ–º–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        self.experiment_log = []
        self.model_versions = {}
        
        # –î–∞–Ω–Ω—ã–µ
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.train_data = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.performance_history = []
        self.data_quality_report = {}
        
        print(f"üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ML –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è {task_type}")
    
    def load_data(self, data, target_column):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Parameters:
        -----------
        data : pandas.DataFrame
            –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        target_column : str
            –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        """
        print(f"\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        self.train_data = data.copy()
        self.target_column = target_column
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π, {len(data.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {target_column}")
        
        # Data quality check
        self._check_data_quality(data)
        
        return self
    
    def _check_data_quality(self, data):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        quality_report = {
            'total_rows': len(data),
            'total_columns': len(data.columns),
            'missing_values': {},
            'duplicate_rows': data.duplicated().sum(),
            'data_types': data.dtypes.to_dict(),
            'numeric_columns': list(data.select_dtypes(include=[np.number]).columns),
            'categorical_columns': list(data.select_dtypes(include=['object', 'bool']).columns)
        }
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        missing = data.isnull().sum()
        quality_report['missing_values'] = {col: int(count) for col, count in missing.items() if count > 0}
        
        # –í—ã–±—Ä–æ—Å—ã –≤ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
        outliers_info = {}
        for col in quality_report['numeric_columns']:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR))).sum()
            if outliers > 0:
                outliers_info[col] = int(outliers)
        
        quality_report['outliers'] = outliers_info
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        self.data_quality_report = quality_report
        
        # –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        print(f"  üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {quality_report['total_rows']} —Å—Ç—Ä–æ–∫ √ó {quality_report['total_columns']} —Å—Ç–æ–ª–±—Ü–æ–≤")
        print(f"  üîÑ –î—É–±–ª–∏–∫–∞—Ç—ã: {quality_report['duplicate_rows']} —Å—Ç—Ä–æ–∫")
        
        if quality_report['missing_values']:
            print(f"  ‚ùó –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
            for col, count in quality_report['missing_values'].items():
                percent = count / quality_report['total_rows'] * 100
                print(f"    {col}: {count} ({percent:.1f}%)")
        else:
            print(f"  ‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç")
        
        if outliers_info:
            print(f"  üìä –í—ã–±—Ä–æ—Å—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤ {len(outliers_info)} —Å—Ç–æ–ª–±—Ü–∞—Ö")
        else:
            print(f"  ‚úÖ –ö—Ä–∏—Ç–∏—á–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
    
    def prepare_features(self, custom_features=None, auto_feature_engineering=True):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Parameters:
        -----------
        custom_features : list, optional
            –°–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        auto_feature_engineering : bool
            –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        print(f"\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        data = self.train_data.copy()
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        if custom_features:
            available_features = [f for f in custom_features if f in data.columns and f != self.target_column]
            print(f"  üìã –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(available_features)}")
        else:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∏—Å–∫–ª—é—á–∞–µ–º ID, —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é)
            exclude_patterns = ['id', 'ID', 'Id', self.target_column]
            available_features = [col for col in data.columns 
                                if col not in exclude_patterns and 
                                not any(pattern in col.lower() for pattern in ['id', '_id'])]
            print(f"  ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(available_features)}")
        
        # Feature engineering
        if auto_feature_engineering:
            data = self._auto_feature_engineering(data, available_features)
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            new_features = [col for col in data.columns 
                          if col not in self.train_data.columns and col != self.target_column]
            available_features.extend(new_features)
            print(f"  ‚ú® –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(new_features)}")
        
        self.feature_names = available_features
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ X –∏ y
        X = data[available_features]
        y = data[self.target_column]
        
        # Train-test split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state,
            stratify=y if self.task_type == 'classification' else None
        )
        
        print(f"  üìä –ò—Ç–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(available_features)}")
        print(f"  üéØ Train: {len(self.X_train)}, Test: {len(self.X_test)}")
        
        # –°–æ–∑–¥–∞–µ–º preprocessor
        self._create_preprocessor()
        
        return self
    
    def _auto_feature_engineering(self, data, base_features):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print(f"  ‚ö° –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π feature engineering...")
        
        # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numeric_features = [col for col in base_features 
                           if col in data.columns and data[col].dtype in ['int64', 'float64']]
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        new_data = data.copy()
        created_features = []
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º—ã –¥–ª—è —Å–∫–æ—à–µ–Ω–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        for col in numeric_features:
            if (data[col] > 0).all() and data[col].skew() > 1:
                new_col = f'log_{col}'
                new_data[new_col] = np.log1p(data[col])
                created_features.append(new_col)
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        if len(numeric_features) >= 2:
            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            correlations = abs(data[numeric_features + [self.target_column]].corr()[self.target_column])
            top_features = correlations.drop(self.target_column).nlargest(3).index.tolist()
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
            for i in range(len(top_features)):
                for j in range(i+1, len(top_features)):
                    new_col = f'{top_features[i]}_x_{top_features[j]}'
                    new_data[new_col] = data[top_features[i]] * data[top_features[j]]
                    created_features.append(new_col)
        
        # –ë–∏–Ω–Ω–∏–Ω–≥ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        for col in numeric_features[:3]:  # –¢–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–ø-3
            new_col = f'{col}_binned'
            new_data[new_col] = pd.cut(data[col], bins=5, labels=False)
            created_features.append(new_col)
        
        print(f"    üìà –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(created_features)}")
        return new_data
    
    def _create_preprocessor(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ preprocessor –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        numeric_features = []
        categorical_features = []
        
        for col in self.feature_names:
            if col in self.X_train.columns:
                if self.X_train[col].dtype in ['int64', 'float64']:
                    numeric_features.append(col)
                else:
                    categorical_features.append(col)
        
        # –°–æ–∑–¥–∞–µ–º transformers
        transformers = []
        
        if numeric_features:
            transformers.append(('num', StandardScaler(), numeric_features))
        
        if categorical_features:
            transformers.append(('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), 
                               categorical_features))
        
        if transformers:
            self.preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')
            print(f"  üîß Preprocessor: {len(numeric_features)} —á–∏—Å–ª–æ–≤—ã—Ö, {len(categorical_features)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö")
        else:
            self.preprocessor = None
            print(f"  ‚ö†Ô∏è Preprocessor –Ω–µ –Ω—É–∂–µ–Ω")
    
    def train_models(self, models_to_try=None, hyperparameter_tuning=True):
        """
        –û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        
        Parameters:
        -----------
        models_to_try : list, optional
            –°–ø–∏—Å–æ–∫ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        hyperparameter_tuning : bool
            –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –ª–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if self.preprocessor:
            X_train_processed = self.preprocessor.fit_transform(self.X_train)
            X_test_processed = self.preprocessor.transform(self.X_test)
        else:
            X_train_processed = self.X_train
            X_test_processed = self.X_test
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if models_to_try is None:
            if self.task_type == 'classification':
                models_to_try = ['logistic', 'tree', 'random_forest']
            else:
                models_to_try = ['linear', 'ridge', 'random_forest']
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
        model_configs = self._get_model_configs()
        
        results = {}
        
        for model_name in models_to_try:
            if model_name not in model_configs:
                print(f"  ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
                continue
            
            print(f"  üìà –û–±—É—á–µ–Ω–∏–µ {model_name}...")
            
            # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
            base_model = model_configs[model_name]['model']
            
            if hyperparameter_tuning and 'param_grid' in model_configs[model_name]:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                param_grid = model_configs[model_name]['param_grid']
                
                grid_search = GridSearchCV(
                    base_model, param_grid, cv=3, 
                    scoring='f1' if self.task_type == 'classification' else 'r2',
                    n_jobs=-1
                )
                
                grid_search.fit(X_train_processed, self.y_train)
                model = grid_search.best_estimator_
                
                print(f"    üéØ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
            else:
                # –û–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                model = base_model
                model.fit(X_train_processed, self.y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_test_processed)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            if self.task_type == 'classification':
                metrics = {
                    'accuracy': accuracy_score(self.y_test, y_pred),
                    'precision': precision_score(self.y_test, y_pred, average='weighted'),
                    'recall': recall_score(self.y_test, y_pred, average='weighted'),
                    'f1': f1_score(self.y_test, y_pred, average='weighted')
                }
                
                # ROC-AUC —Ç–æ–ª—å–∫–æ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                if len(np.unique(self.y_train)) == 2:
                    y_proba = model.predict_proba(X_test_processed)[:, 1]
                    metrics['roc_auc'] = roc_auc_score(self.y_test, y_proba)
                
            else:
                metrics = {
                    'r2': r2_score(self.y_test, y_pred),
                    'mse': mean_squared_error(self.y_test, y_pred),
                    'rmse': np.sqrt(mean_squared_error(self.y_test, y_pred)),
                    'mae': mean_absolute_error(self.y_test, y_pred)
                }
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_score = cross_val_score(
                model, X_train_processed, self.y_train, cv=3,
                scoring='f1_weighted' if self.task_type == 'classification' else 'r2'
            )
            metrics['cv_score'] = cv_score.mean()
            metrics['cv_std'] = cv_score.std()
            
            results[model_name] = {
                'model': model,
                'metrics': metrics,
                'predictions': y_pred
            }
            
            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if self.task_type == 'classification':
                print(f"    üìä F1: {metrics['f1']:.3f}, CV: {metrics['cv_score']:.3f}¬±{metrics['cv_std']:.3f}")
            else:
                print(f"    üìä R¬≤: {metrics['r2']:.3f}, CV: {metrics['cv_score']:.3f}¬±{metrics['cv_std']:.3f}")
        
        self.models = results
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if self.task_type == 'classification':
            best_model_name = max(results.keys(), key=lambda x: results[x]['metrics']['f1'])
        else:
            best_model_name = max(results.keys(), key=lambda x: results[x]['metrics']['r2'])
        
        self.best_model = results[best_model_name]['model']
        self.best_model_name = best_model_name
        
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        self._log_experiment(results)
        
        return self
    
    def _get_model_configs(self):
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        configs = {}
        
        if self.task_type == 'classification':
            configs['logistic'] = {
                'model': LogisticRegression(random_state=self.random_state, max_iter=1000),
                'param_grid': {
                    'C': [0.1, 1.0, 10.0],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear']
                }
            }
            
            configs['tree'] = {
                'model': DecisionTreeClassifier(random_state=self.random_state),
                'param_grid': {
                    'max_depth': [5, 10, 15, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            }
            
            configs['random_forest'] = {
                'model': RandomForestClassifier(random_state=self.random_state, n_jobs=-1),
                'param_grid': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [10, 15, 20, None],
                    'min_samples_split': [2, 5, 10]
                }
            }
            
        else:  # regression
            configs['linear'] = {
                'model': LinearRegression(),
                'param_grid': {}
            }
            
            configs['ridge'] = {
                'model': Ridge(random_state=self.random_state),
                'param_grid': {
                    'alpha': [0.1, 1.0, 10.0, 100.0]
                }
            }
            
            configs['lasso'] = {
                'model': Lasso(random_state=self.random_state),
                'param_grid': {
                    'alpha': [0.1, 1.0, 10.0, 100.0]
                }
            }
            
            configs['random_forest'] = {
                'model': RandomForestRegressor(random_state=self.random_state, n_jobs=-1),
                'param_grid': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [10, 15, 20, None],
                    'min_samples_split': [2, 5, 10]
                }
            }
        
        return configs
    
    def _log_experiment(self, results):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        experiment = {
            'timestamp': datetime.now().isoformat(),
            'task_type': self.task_type,
            'feature_count': len(self.feature_names),
            'train_size': len(self.X_train),
            'test_size': len(self.X_test),
            'models': {name: result['metrics'] for name, result in results.items()},
            'best_model': self.best_model_name,
            'data_quality': self.data_quality_report
        }
        
        self.experiment_log.append(experiment)
        print(f"  üìù –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω ({len(self.experiment_log)} –≤—Å–µ–≥–æ)")
    
    def evaluate_model(self, detailed=True):
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏"""
        if not self.best_model:
            print("‚ùå –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å!")
            return
        
        print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {self.best_model_name}")
        print("=" * 50)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if self.preprocessor:
            X_test_processed = self.preprocessor.transform(self.X_test)
        else:
            X_test_processed = self.X_test
        
        y_pred = self.best_model.predict(X_test_processed)
        
        if self.task_type == 'classification':
            self._evaluate_classification(y_pred, detailed)
        else:
            self._evaluate_regression(y_pred, detailed)
        
        if detailed:
            self._plot_feature_importance()
            self._create_evaluation_plots(y_pred)
    
    def _evaluate_classification(self, y_pred, detailed):
        """–û—Ü–µ–Ω–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        from sklearn.metrics import classification_report, confusion_matrix
        
        print("üéØ –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(self.y_test, y_pred))
        
        if detailed:
            print("üîç –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
            cm = confusion_matrix(self.y_test, y_pred)
            print(cm)
    
    def _evaluate_regression(self, y_pred, detailed):
        """–û—Ü–µ–Ω–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        r2 = r2_score(self.y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
        mae = mean_absolute_error(self.y_test, y_pred)
        mape = np.mean(np.abs((self.y_test - y_pred) / self.y_test)) * 100
        
        print(f"üéØ –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:")
        print(f"  R¬≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏): {r2:.3f}")
        print(f"  RMSE (—Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞): {rmse:.2f}")
        print(f"  MAE (—Å—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞): {mae:.2f}")
        print(f"  MAPE (—Å—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è % –æ—à–∏–±–∫–∞): {mape:.1f}%")
    
    def _plot_feature_importance(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if hasattr(self.best_model, 'feature_importances_'):
            # –î–ª—è –¥—Ä–µ–≤–µ—Å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            importances = self.best_model.feature_importances_
            feature_names = self.feature_names
            
        elif hasattr(self.best_model, 'coef_'):
            # –î–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            importances = np.abs(self.best_model.coef_)
            feature_names = self.feature_names
            
            # –î–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ coef_ (multiclass)
            if importances.ndim > 1:
                importances = np.mean(np.abs(importances), axis=0)
                
        else:
            print("  ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=True)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-10
        plt.figure(figsize=(10, 6))
        top_features = importance_df.tail(10)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.title(f'üéØ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({self.best_model_name})')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
        plt.tight_layout()
        plt.show()
    
    def _create_evaluation_plots(self, y_pred):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        if self.task_type == 'classification':
            # ROC-–∫—Ä–∏–≤–∞—è –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if len(np.unique(self.y_train)) == 2:
                from sklearn.metrics import roc_curve
                
                if self.preprocessor:
                    X_test_processed = self.preprocessor.transform(self.X_test)
                else:
                    X_test_processed = self.X_test
                
                y_proba = self.best_model.predict_proba(X_test_processed)[:, 1]
                fpr, tpr, _ = roc_curve(self.y_test, y_proba)
                
                axes[0].plot(fpr, tpr, linewidth=2)
                axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)
                axes[0].set_xlabel('False Positive Rate')
                axes[0].set_ylabel('True Positive Rate')
                axes[0].set_title('üìà ROC-–∫—Ä–∏–≤–∞—è')
                axes[0].grid(alpha=0.3)
            
            # Confusion matrix
            from sklearn.metrics import confusion_matrix
            import seaborn as sns
            
            cm = confusion_matrix(self.y_test, y_pred)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])
            axes[1].set_title('üîç –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
            axes[1].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
            axes[1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
            
        else:
            # Predicted vs Actual
            axes[0].scatter(self.y_test, y_pred, alpha=0.6)
            min_val, max_val = min(self.y_test.min(), y_pred.min()), max(self.y_test.max(), y_pred.max())
            axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
            axes[0].set_xlabel('–†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
            axes[0].set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
            axes[0].set_title('üìä –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω–æ—Å—Ç—å')
            axes[0].grid(alpha=0.3)
            
            # Residuals plot
            residuals = self.y_test - y_pred
            axes[1].scatter(y_pred, residuals, alpha=0.6)
            axes[1].axhline(y=0, color='r', linestyle='--')
            axes[1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
            axes[1].set_ylabel('–û—Å—Ç–∞—Ç–∫–∏')
            axes[1].set_title('üìà –ì—Ä–∞—Ñ–∏–∫ –æ—Å—Ç–∞—Ç–∫–æ–≤')
            axes[1].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def predict(self, new_data):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Parameters:
        -----------
        new_data : pandas.DataFrame
            –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
        --------
        predictions : array
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        """
        if not self.best_model:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å!")
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –µ—Å—Ç—å –≤—Å–µ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        missing_features = set(self.feature_names) - set(new_data.columns)
        if missing_features:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_new = new_data[self.feature_names]
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        if self.preprocessor:
            X_new_processed = self.preprocessor.transform(X_new)
        else:
            X_new_processed = X_new
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = self.best_model.predict(X_new_processed)
        
        return predictions
    
    def predict_proba(self, new_data):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        if self.task_type != 'classification':
            raise ValueError("predict_proba –¥–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        
        if not hasattr(self.best_model, 'predict_proba'):
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_new = new_data[self.feature_names]
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        if self.preprocessor:
            X_new_processed = self.preprocessor.transform(X_new)
        else:
            X_new_processed = X_new
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        probabilities = self.best_model.predict_proba(X_new_processed)
        
        return probabilities
    
    def save_model(self, filepath):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.best_model:
            raise ValueError("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")
        
        model_package = {
            'model': self.best_model,
            'preprocessor': self.preprocessor,
            'feature_names': self.feature_names,
            'task_type': self.task_type,
            'best_model_name': self.best_model_name,
            'experiment_log': self.experiment_log,
            'data_quality_report': self.data_quality_report,
            'target_column': self.target_column
        }
        
        joblib.dump(model_package, filepath)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
    
    def load_model(self, filepath):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_package = joblib.load(filepath)
        
        self.best_model = model_package['model']
        self.preprocessor = model_package['preprocessor']
        self.feature_names = model_package['feature_names']
        self.task_type = model_package['task_type']
        self.best_model_name = model_package['best_model_name']
        self.experiment_log = model_package.get('experiment_log', [])
        self.data_quality_report = model_package.get('data_quality_report', {})
        self.target_column = model_package.get('target_column')
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")
        print(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {self.task_type}")
        print(f"üèÜ –ú–æ–¥–µ–ª—å: {self.best_model_name}")
        print(f"üìä –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_names) if self.feature_names else 0}")
    
    def get_model_summary(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –æ –º–æ–¥–µ–ª–∏"""
        if not self.best_model:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
            return None
        
        summary = {
            'model_name': self.best_model_name,
            'task_type': self.task_type,
            'feature_count': len(self.feature_names) if self.feature_names else 0,
            'train_size': len(self.X_train) if self.X_train is not None else 0,
            'test_size': len(self.X_test) if self.X_test is not None else 0,
            'best_metrics': self.models[self.best_model_name]['metrics'] if self.models else {},
            'data_quality_issues': len(self.data_quality_report.get('missing_values', {})) + 
                                 len(self.data_quality_report.get('outliers', {})),
            'experiments_count': len(self.experiment_log)
        }
        
        return summary
    
    def create_business_report(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–æ—Ç—á–µ—Ç–∞ –æ –º–æ–¥–µ–ª–∏"""
        print(f"\nüíº –ë–ò–ó–ù–ï–°-–û–¢–ß–ï–¢ –û ML –ú–û–î–ï–õ–ò")
        print("=" * 50)
        
        summary = self.get_model_summary()
        if not summary:
            return
        
        print(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {summary['task_type']}")
        print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {summary['model_name']}")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {summary['feature_count']}")
        print(f"üé≤ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {summary['train_size']} train + {summary['test_size']} test")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        metrics = summary['best_metrics']
        if self.task_type == 'classification':
            print(f"\nüìà –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏:")
            print(f"  ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {metrics.get('accuracy', 0):.1%}")
            print(f"  ‚Ä¢ F1-score: {metrics.get('f1', 0):.3f}")
            if 'roc_auc' in metrics:
                print(f"  ‚Ä¢ AUC-ROC: {metrics.get('roc_auc', 0):.3f}")
                
            # –ë–∏–∑–Ω–µ—Å-–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            accuracy = metrics.get('accuracy', 0)
            if accuracy > 0.9:
                quality = "–û—Ç–ª–∏—á–Ω–æ–µ"
            elif accuracy > 0.8:
                quality = "–•–æ—Ä–æ—à–µ–µ"
            elif accuracy > 0.7:
                quality = "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ"
            else:
                quality = "–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
                
            print(f"  ‚Ä¢ –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {quality}")
            
        else:  # regression
            print(f"\nüìà –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏:")
            print(f"  ‚Ä¢ R¬≤ (–æ–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è): {metrics.get('r2', 0):.1%}")
            print(f"  ‚Ä¢ RMSE: {metrics.get('rmse', 0):,.0f}")
            print(f"  ‚Ä¢ MAE: {metrics.get('mae', 0):,.0f}")
            
            # –ë–∏–∑–Ω–µ—Å-–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            r2 = metrics.get('r2', 0)
            if r2 > 0.9:
                quality = "–û—Ç–ª–∏—á–Ω–æ–µ"
            elif r2 > 0.7:
                quality = "–•–æ—Ä–æ—à–µ–µ"
            elif r2 > 0.5:
                quality = "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ"
            else:
                quality = "–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
                
            print(f"  ‚Ä¢ –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {quality}")
        
        # –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        if summary['data_quality_issues'] > 0:
            print(f"\n‚ö†Ô∏è –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
            print(f"  ‚Ä¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {summary['data_quality_issues']} –ø—Ä–æ–±–ª–µ–º —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö")
            print(f"  ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏")
        else:
            print(f"\n‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"  ‚Ä¢ –ü—Ä–æ–≤–µ–¥–µ–Ω–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {summary['experiments_count']}")
        
        if self.task_type == 'classification':
            if metrics.get('f1', 0) < 0.7:
                print(f"  ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏ —É–ª—É—á—à–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏")
            print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        else:
            if metrics.get('r2', 0) < 0.7:
                print(f"  ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
            print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π")
        
        print(f"  ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3-6 –º–µ—Å—è—Ü–µ–≤")

def demo_classification_pipeline():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Classification Pipeline")
    print("=" * 40)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–∫—É–ø–∫–∞—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
        data = pd.read_csv('customer_purchase_prediction.csv')
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {len(data)} –∫–ª–∏–µ–Ω—Ç–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        pipeline = MLPipeline(task_type='classification')
        
        results = (pipeline
                  .load_data(data, 'will_purchase')
                  .prepare_features(auto_feature_engineering=True)
                  .train_models(hyperparameter_tuning=True)
                  .evaluate_model(detailed=True))
        
        # –ë–∏–∑–Ω–µ—Å-–æ—Ç—á–µ—Ç
        pipeline.create_business_report()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        pipeline.save_model('classification_model.pkl')
        
        print(f"\nüéâ –ü–∞–π–ø–ª–∞–π–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return pipeline
        
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª customer_purchase_prediction.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ classification_basics.py")
        return None

def demo_regression_pipeline():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
    print("\nüöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Regression Pipeline")  
    print("=" * 40)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
        data = pd.read_csv('real_estate_prices.csv')
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {len(data)} –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏")
        
        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        pipeline = MLPipeline(task_type='regression')
        
        results = (pipeline
                  .load_data(data, 'price')
                  .prepare_features(auto_feature_engineering=True)
                  .train_models(hyperparameter_tuning=True)
                  .evaluate_model(detailed=True))
        
        # –ë–∏–∑–Ω–µ—Å-–æ—Ç—á–µ—Ç
        pipeline.create_business_report()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        pipeline.save_model('regression_model.pkl')
        
        print(f"\nüéâ –ü–∞–π–ø–ª–∞–π–Ω —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return pipeline
        
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª real_estate_prices.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ regression_models.py")
        return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ production –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    print("üéØ Production-ready ML Pipeline Demo")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
    classification_pipeline = demo_classification_pipeline()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º —Ä–µ–≥—Ä–µ—Å—Å–∏—é  
    regression_pipeline = demo_regression_pipeline()
    
    if classification_pipeline and regression_pipeline:
        print(f"\nüèÜ –û–±–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
        print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"  ‚Ä¢ classification_model.pkl")
        print(f"  ‚Ä¢ regression_model.pkl")
        print(f"\nüí° –ú–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –¥–ª—è production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è!")
        
        return classification_pipeline, regression_pipeline
    
    return None, None

if __name__ == "__main__":
    classification_pipeline, regression_pipeline = main()